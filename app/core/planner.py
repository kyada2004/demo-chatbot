import json
import re
from datetime import datetime
from langdetect import detect
from deep_translator import GoogleTranslator

from app.features.ai import get_ai_response
from app.database.preferences import get_user_preference

class PlanningEngine:
    """
    Data Flow Step 2: Intent Detection
    The PlanningEngine uses a large language model (LLM) to analyze the user's query
    and determine the appropriate tool(s) to use from a predefined list.
    It generates a structured plan (a list of steps) to be executed.
    """
    def __init__(self, agent):
        self.agent = agent

    def get_tools_definition(self):
        tools = []
        for intent, data in self.agent.intent_routes.items():
            tools.append({
                "name": intent,
                "description": data.get("description", f"Handles {intent}"),
                "args": data.get("args", {})
            })
        return json.dumps(tools, indent=2)

    async def create_plan(self, query):
        tools_definition = self.get_tools_definition()
        
        prompt = f"""You are an intelligent planner for an AI assistant named Jenny.
Your job is to create a step-by-step plan to fulfill the user's request by using the available tools.

**Available Tools:**
{tools_definition}

**User Query:**
"{query}"

**Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}

**Instructions:**
1.  Analyze the user's query to understand their intent.
2.  If the query requires information from the user's files, you MUST use the "file_query" tool.
3.  If the query can be fulfilled using one or more tools, create a plan.
4.  The plan must be a JSON array of objects. Each object represents a step and must have a "tool_name" and an "args" object.
5.  The "tool_name" must be one of the available tools.
6.  The "args" object should contain the arguments for the tool, extracted from the user's query.
7.  If a tool requires no arguments, provide an empty "args" object: {{}}.
8.  If the query is a simple question, a greeting, or something that doesn't require a tool, return an empty JSON array `[]`. The main AI will handle it.
9.  Only return the JSON array, with no other text before or after it.

**Plan:**
"""
        
        response = await get_ai_response(prompt, self.agent.app.message_history, self.agent.ai_brain_context, stream=False)
        return response

    async def execute_plan(self, query, response_queue):
        """
        Data Flow Step 3: Decision (Core Logic)
        This method executes the plan generated by create_plan.
        It iterates through the steps, calls the appropriate handler function for each tool,
        and passes the required arguments.
        If no plan is generated, it falls back to a general AI response.
        """
        # 1. Handle conversational context (moved from agent)
        if self.agent.conversation_context == "AWAITING_CITY_FOR_WEATHER":
            self.agent.conversation_context = None
            self.agent.handle_weather({"city": query})
            return
        # ... (add other conversational context handlers here if any) ...

        # 2. Translate query to English (moved from agent)
        try:
            lang = detect(query)
            if lang != "en":
                query = GoogleTranslator(source='auto', target='en').translate(query)
        except Exception as e:
            print(f"[Translation Error] {e}")
            pass

        try:
            # 3. Create a plan using the LLM
            plan_response = await self.create_plan(query)
            plan_str = plan_response.get('text', '')
            
            match = re.search(r'\[.*\]', plan_str, re.DOTALL)
            if not match:
                response_generator = await get_ai_response(query, self.agent.app.message_history, self.agent.ai_brain_context, rag_context=rag_context, stream=True)
                async for chunk in response_generator:
                    response_queue.put(chunk)
                response_queue.put(None)
                return
                
            plan_str = match.group(0)
            plan = json.loads(plan_str)

            if not plan:
                # If plan is empty, check for RAG context before falling back to AI
                rag_context = self.agent.rag.retrieve_context(query)
                response_generator = get_ai_response(query, self.agent.app.message_history, self.agent.ai_brain_context, rag_context=rag_context, stream=True)
                async for chunk in response_generator:
                    response_queue.put(chunk)
                response_queue.put(None)
                return

            # 4. Execute the plan
            responses = []
            for step in plan:
                tool_name = step.get("tool_name")
                args = step.get("args", {})
                
                if tool_name in self.agent.intent_routes:
                    handler = self.agent.intent_routes[tool_name]["handler"]
                    
                    # Special handling for async and response_queue-based handlers
                    if tool_name in ["google_search", "file_query", "research_and_summarize"]:
                        await handler(args, response_queue)
                        return 
                    else:
                        if asyncio.iscoroutinefunction(handler):
                            result = await handler(args)
                        else:
                            result = handler(args)
                        responses.append(result)
                else:
                    responses.append(f"Sorry, I found an unknown tool in my plan: {tool_name}")
            
            response_text = "\n".join(map(str, responses))
            response_queue.put(response_text)
            response_queue.put(None)

        except Exception as e:
            print(f"[Planning Error] {e}")
            # Fallback to AI with RAG context if planning fails
            rag_context = self.agent.rag.retrieve_context(query)
            response_generator = await get_ai_response(query, self.agent.app.message_history, self.agent.ai_brain_context, rag_context=rag_context, stream=True)
            async for chunk in response_generator:
                response_queue.put(chunk)
            response_queue.put(None)